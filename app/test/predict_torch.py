import argparse
import os
import librosa
import numpy as np
import torch
import torch.nn as nn
from torchvision import models

# -----------------------------------------------------------------------------
# ã‚¹ã‚¯ãƒªãƒ—ãƒˆã®èª¬æ˜ã¨ä½¿ã„æ–¹
# -----------------------------------------------------------------------------
#
# ã“ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¯ã€å­¦ç¿’æ¸ˆã¿ã®PyTorchãƒ¢ãƒ‡ãƒ«(.pth)ã‚’ä½¿ç”¨ã—ã¦ã€
# æŒ‡å®šã•ã‚ŒãŸå˜ä¸€ã®éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰åˆ†é¡çµæœã‚’æ¨è«–ã—ã¾ã™ã€‚
#
# ä½¿ã„æ–¹ (ã‚¿ãƒ¼ãƒŸãƒŠãƒ«ã§å®Ÿè¡Œ):
# python predict_pytorch.py --model_path path/to/your/best_model.pth --audio_path path/to/your/audio.wav
#
# ä¾‹:
# python predict_pytorch.py --model_path best_pytorch_model.pth --audio_path my_sounds/light_on.wav
#

# -----------------------------------------------------------------------------
# ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®š (å­¦ç¿’æ™‚ã¨å®Œå…¨ã«ä¸€è‡´ã•ã›ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™)
# -----------------------------------------------------------------------------
SAMPLE_RATE = 16000
DURATION_SECONDS = 5
N_MELS = 128
NUM_CLASSES = 3

# ãƒ‡ãƒã‚¤ã‚¹è¨­å®š
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# -----------------------------------------------------------------------------
# ãƒ¢ãƒ‡ãƒ«ã‚¯ãƒ©ã‚¹ã®å®šç¾© (å­¦ç¿’æ™‚ã¨åŒä¸€ã®ã‚‚ã®ã‚’è¨˜è¿°)
# -----------------------------------------------------------------------------

class AudioClassifier(nn.Module):
    """PyTorchã®ã‚«ã‚¹ã‚¿ãƒ ãƒ¢ãƒ‡ãƒ«ã‚¯ãƒ©ã‚¹"""
    def __init__(self, num_classes=3):
        super(AudioClassifier, self).__init__()
        # PyTorch 1.9ä»¥é™ã§ã¯ weights ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒæ¨å¥¨ã•ã‚Œã¦ã„ã¾ã™
        self.mobilenet = models.mobilenet_v2(weights=models.MobileNet_V2_Weights.DEFAULT)
        
        # 1. å…¥åŠ›å±¤ã®å¤‰æ›´ (1ãƒãƒ£ãƒãƒ« -> 3ãƒãƒ£ãƒãƒ«)
        original_first_layer = self.mobilenet.features[0][0]
        self.mobilenet.features[0][0] = nn.Conv2d(1, 32, kernel_size=3, stride=2, padding=1, bias=False)
        with torch.no_grad():
             self.mobilenet.features[0][0].weight.data = original_first_layer.weight.data.mean(dim=1, keepdim=True)

        # 2. å‡ºåŠ›å±¤ã®å¤‰æ›´
        num_ftrs = self.mobilenet.classifier[1].in_features
        self.mobilenet.classifier = nn.Sequential(
            nn.Dropout(p=0.3),
            nn.Linear(num_ftrs, num_classes)
        )

    def forward(self, x):
        return self.mobilenet(x)

# -----------------------------------------------------------------------------
# å‰å‡¦ç†é–¢æ•°
# -----------------------------------------------------------------------------

def preprocess_audio_for_prediction(file_path):
    """
    æ¨è«–ç”¨ã«å˜ä¸€ã®éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰å‡¦ç†ã—ã€PyTorchãƒ†ãƒ³ã‚½ãƒ«ã‚’è¿”ã™ã€‚
    """
    try:
        y, sr = librosa.load(file_path, sr=SAMPLE_RATE, duration=DURATION_SECONDS, res_type='kaiser_fast')
        if librosa.get_duration(y=y, sr=sr) < DURATION_SECONDS:
            y = librosa.util.fix_length(y, size=SAMPLE_RATE * DURATION_SECONDS)
        
        melspec = librosa.feature.melspectrogram(y=y, sr=SAMPLE_RATE, n_mels=N_MELS)
        melspec_db = librosa.power_to_db(melspec, ref=np.max)
        
        # PyTorchã®ãƒ†ãƒ³ã‚½ãƒ«å½¢å¼ (C, H, W) ã«å¤‰æ›
        tensor = torch.from_numpy(melspec_db).float().unsqueeze(0)
        
        # ãƒãƒƒãƒæ¬¡å…ƒã‚’è¿½åŠ  (B, C, H, W)
        return tensor.unsqueeze(0)
    except Exception as e:
        print(f"Error processing file {file_path}: {e}")
        return None

# -----------------------------------------------------------------------------
# ãƒ¡ã‚¤ãƒ³ã®å®Ÿè¡Œãƒ–ãƒ­ãƒƒã‚¯
# -----------------------------------------------------------------------------

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Classify an audio file using a trained PyTorch model.")
    parser.add_argument("--model_path", type=str, required=True, help="Path to the trained .pth model file.")
    parser.add_argument("--audio_path", type=str, required=True, help="Path to the .wav audio file to classify.")
    args = parser.parse_args()

    if not os.path.exists(args.model_path):
        print(f"Error: Model file not found at {args.model_path}")
        exit()
    if not os.path.exists(args.audio_path):
        print(f"Error: Audio file not found at {args.audio_path}")
        exit()

    # 1. ãƒ¢ãƒ‡ãƒ«ã®éª¨æ ¼ã‚’ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åŒ–
    model = AudioClassifier(num_classes=NUM_CLASSES).to(DEVICE)

    # 2. å­¦ç¿’æ¸ˆã¿ã®é‡ã¿ï¼ˆçŠ¶æ…‹è¾æ›¸ï¼‰ã‚’ãƒ­ãƒ¼ãƒ‰
    print(f"Loading model weights from: {args.model_path}")
    try:
        model.load_state_dict(torch.load(args.model_path, map_location=DEVICE))
    except Exception as e:
        print(f"Error loading model weights: {e}")
        exit()
        
    # 3. ãƒ¢ãƒ‡ãƒ«ã‚’è©•ä¾¡ãƒ¢ãƒ¼ãƒ‰ã«è¨­å®š (é‡è¦ï¼)
    #    Dropoutãªã©ã®å±¤ã‚’ç„¡åŠ¹åŒ–ã—ã¾ã™ã€‚
    model.eval()

    # 4. å…¥åŠ›éŸ³å£°ã‚’å‰å‡¦ç†
    input_tensor = preprocess_audio_for_prediction(args.audio_path)

    if input_tensor is not None:
        input_tensor = input_tensor.to(DEVICE)

        # 5. æ¨è«–ã‚’å®Ÿè¡Œ (å‹¾é…è¨ˆç®—ã‚’ç„¡åŠ¹åŒ–ã—ã¦é«˜é€ŸåŒ–)
        print("Running prediction...")
        with torch.no_grad():
            outputs = model(input_tensor)
        
        # 6. çµæœã‚’è§£é‡ˆã—ã¦è¡¨ç¤º
        # Softmaxã‚’é©ç”¨ã—ã¦ç¢ºç‡ã«å¤‰æ›
        probabilities = torch.nn.functional.softmax(outputs[0], dim=0)
        
        confidence, predicted_class_index = torch.max(probabilities, 0)
        
        class_names = {0: "OFF", 1: "ON", 2: "KEEP"}
        predicted_label = class_names.get(predicted_class_index.item(), "Unknown")
        
        print("\n--- ğŸ’¡ æ¨è«–çµæœ ---")
        print(f"  äºˆæ¸¬ãƒ©ãƒ™ãƒ«: {predicted_label} (ã‚¯ãƒ©ã‚¹ID: {predicted_class_index.item()})")
        print(f"  ä¿¡é ¼åº¦: {confidence.item():.2%}")
        print("--------------------")