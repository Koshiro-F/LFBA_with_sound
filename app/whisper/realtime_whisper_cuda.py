import sounddevice as sd
import numpy as np
import wave
import tempfile
import os
import threading
import time
from datetime import datetime
from collections import deque
import signal
import sys
import torch
import scipy.signal
import platform

class EnhancedRealtimeWhisperTranscriber:
    def __init__(self, mic_device, model_name="openai/whisper-base", chunk_duration=3, method="transformers"):
        self.mic_device = mic_device
        self.model_name = model_name
        self.chunk_duration = chunk_duration
        self.method = method
        
        # OSæ¤œå‡º
        self.os_name = platform.system()
        print(f"ğŸ–¥ï¸  æ¤œå‡ºã•ã‚ŒãŸOS: {self.os_name}")
        
        # éŸ³å£°è¨­å®šï¼ˆå‹•çš„ã«èª¿æ•´ï¼‰
        self.target_fs = 16000  # Whisperã«æœ€é©ãªã‚µãƒ³ãƒ—ãƒ«ãƒ¬ãƒ¼ãƒˆ
        self.fs = self._detect_supported_sample_rate()  # å®Ÿéš›ã®éŒ²éŸ³ã‚µãƒ³ãƒ—ãƒ«ãƒ¬ãƒ¼ãƒˆ
        self.channels = 1
        self.nyquist_freq = self.fs / 2
        
        # GPUè¨­å®š
        self.device = self._check_device()
        
        # çŠ¶æ…‹ç®¡ç†
        self.is_running = False
        self.stop_event = threading.Event()
        
        # éŸ³å£°ãƒãƒƒãƒ•ã‚¡
        self.audio_buffer = deque()
        self.buffer_lock = threading.Lock()
        
        # ã‚¹ãƒ¬ãƒƒãƒ‰
        self.recording_thread = None
        self.transcription_thread = None
        
        # ãƒ¢ãƒ‡ãƒ«
        self.model = None
        self.processor = None
        
        # çµæœä¿å­˜
        self.transcriptions = []
        
        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹çµ±è¨ˆ
        self.processing_times = []
        
        # VADã¨ãƒã‚¤ã‚ºãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°è¨­å®šï¼ˆå®‰å…¨ãªå‘¨æ³¢æ•°ç¯„å›²ï¼‰
        self.vad_threshold = 0.02
        self.min_speech_duration = 0.5
        self.noise_gate_threshold = 0.01
        self.background_noise_level = 0.005
        
        # ãƒ•ã‚£ãƒ«ã‚¿è¨­å®šï¼ˆå‹•çš„ã«èª¿æ•´ï¼‰
        self.highpass_freq = max(50, 0.01 * self.nyquist_freq)
        self.lowpass_freq = min(min(7000, self.target_fs//2 - 100), 0.875 * self.nyquist_freq)
        
        # ãƒã‚¤ã‚ºãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«
        self.noise_profile = None
        self.noise_samples = []
        
        # çµ±è¨ˆæƒ…å ±
        self.total_chunks = 0
        self.speech_chunks = 0
        self.filtered_chunks = 0
        
        print("ğŸ¯ éŸ³å£°ã‚¢ã‚¯ãƒ†ã‚£ãƒ“ãƒ†ã‚£æ¤œå‡ºï¼ˆVADï¼‰æ©Ÿèƒ½ã‚’æœ‰åŠ¹ã«ã—ã¾ã—ãŸ")
        print("ğŸ”‡ ãƒã‚¤ã‚ºãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°æ©Ÿèƒ½ã‚’æœ‰åŠ¹ã«ã—ã¾ã—ãŸ")
        print(f"ğŸ”§ ãƒ•ã‚£ãƒ«ã‚¿å‘¨æ³¢æ•°ç¯„å›²: {self.highpass_freq:.0f}Hz - {self.lowpass_freq:.0f}Hz")
        print(f"ğŸ™ï¸  éŒ²éŸ³ã‚µãƒ³ãƒ—ãƒ«ãƒ¬ãƒ¼ãƒˆ: {self.fs}Hz â†’ Whisperç”¨: {self.target_fs}Hz")
    
    def _detect_supported_sample_rate(self):
        """ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ã‚‹ã‚µãƒ³ãƒ—ãƒ«ãƒ¬ãƒ¼ãƒˆã‚’æ¤œå‡º"""
        # ä¸€èˆ¬çš„ãªã‚µãƒ³ãƒ—ãƒ«ãƒ¬ãƒ¼ãƒˆã‚’è©¦è¡Œé †ã«ãƒªã‚¹ãƒˆ
        sample_rates = [16000, 44100, 48000, 22050, 32000, 8000]
        
        print("ğŸ” ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ã‚‹ã‚µãƒ³ãƒ—ãƒ«ãƒ¬ãƒ¼ãƒˆã‚’æ¤œå‡ºä¸­...")
        
        for rate in sample_rates:
            try:
                # ãƒ†ã‚¹ãƒˆéŒ²éŸ³ï¼ˆéå¸¸ã«çŸ­æ™‚é–“ï¼‰
                test_duration = 0.1  # 100ms
                test_recording = sd.rec(
                    int(test_duration * rate), 
                    samplerate=rate, 
                    channels=1, 
                    dtype='int16', 
                    device=self.mic_device
                )
                sd.wait()
                
                print(f"âœ… ã‚µãƒ³ãƒ—ãƒ«ãƒ¬ãƒ¼ãƒˆ {rate}Hz: ã‚µãƒãƒ¼ãƒˆæ¸ˆã¿")
                return rate
                
            except Exception as e:
                print(f"âŒ ã‚µãƒ³ãƒ—ãƒ«ãƒ¬ãƒ¼ãƒˆ {rate}Hz: éã‚µãƒãƒ¼ãƒˆ ({str(e)[:50]}...)")
                continue
        
        # ã©ã®ã‚µãƒ³ãƒ—ãƒ«ãƒ¬ãƒ¼ãƒˆã‚‚å‹•ä½œã—ãªã„å ´åˆ
        print("âš ï¸  ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ã‚‹ã‚µãƒ³ãƒ—ãƒ«ãƒ¬ãƒ¼ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
        return 16000
    
    def _check_device(self):
        """GPUåˆ©ç”¨å¯èƒ½æ€§ã‚’ãƒã‚§ãƒƒã‚¯ï¼ˆCUDAå„ªå…ˆï¼‰"""
        if torch.cuda.is_available():
            gpu_count = torch.cuda.device_count()
            gpu_name = torch.cuda.get_device_name(0) if gpu_count > 0 else "Unknown"
            print(f"âœ… NVIDIA GPU (CUDA) ãŒåˆ©ç”¨å¯èƒ½ã§ã™")
            print(f"   GPUæ•°: {gpu_count}, GPUå: {gpu_name}")
            return "cuda"
        elif torch.backends.mps.is_available():
            print("âœ… Apple Silicon GPU (MPS) ãŒåˆ©ç”¨å¯èƒ½ã§ã™")
            return "mps"
        else:
            print("âš ï¸  GPU ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚CPUã‚’ä½¿ç”¨ã—ã¾ã™")
            return "cpu"
    
    def resample_audio(self, audio_data, original_fs, target_fs):
        """éŸ³å£°ã®ãƒªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°"""
        if original_fs == target_fs:
            return audio_data
        
        try:
            # scipy.signalã‚’ä½¿ç”¨ã—ã¦ãƒªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
            num_samples = int(len(audio_data) * target_fs / original_fs)
            resampled = scipy.signal.resample(audio_data, num_samples)
            return resampled.astype(np.int16)
        except Exception as e:
            print(f"ãƒªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã‚¨ãƒ©ãƒ¼: {e}")
            return audio_data
    
    def update_noise_profile(self, audio_chunk):
        """ãƒã‚¤ã‚ºãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ã‚’é©å¿œçš„ã«æ›´æ–°"""
        try:
            rms = np.sqrt(np.mean(audio_chunk.astype(np.float32) ** 2))
            
            if rms < self.noise_gate_threshold:
                self.noise_samples.append(audio_chunk)
                
                if len(self.noise_samples) > 50:
                    self.noise_samples.pop(0)
                
                if len(self.noise_samples) >= 5:
                    combined_noise = np.concatenate(self.noise_samples)
                    self.noise_profile = np.mean(combined_noise)
                    self.background_noise_level = np.std(combined_noise) * 2
        except Exception as e:
            print(f"ãƒã‚¤ã‚ºãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«æ›´æ–°ã‚¨ãƒ©ãƒ¼: {e}")
    
    def apply_noise_reduction(self, audio_chunk):
        """ã‚¹ãƒšã‚¯ãƒˆãƒ«æ¸›ç®—ã«ã‚ˆã‚‹ãƒã‚¤ã‚ºé™¤å»"""
        try:
            if len(audio_chunk) < 256:
                return audio_chunk
            
            nperseg = min(256, len(audio_chunk) // 4)
            if nperseg < 4:
                return audio_chunk
            
            f, t, stft = scipy.signal.stft(audio_chunk, fs=self.target_fs, nperseg=nperseg)
            
            magnitude = np.abs(stft)
            phase = np.angle(stft)
            
            if self.noise_profile is not None:
                noise_power = self.background_noise_level ** 2
                signal_power = magnitude ** 2
                
                alpha = 1.5
                enhanced_magnitude = magnitude * np.maximum(
                    0.1, 1 - alpha * noise_power / (signal_power + 1e-10)
                )
            else:
                enhanced_magnitude = magnitude
            
            enhanced_stft = enhanced_magnitude * np.exp(1j * phase)
            _, enhanced_audio = scipy.signal.istft(enhanced_stft, fs=self.target_fs, nperseg=nperseg)
            
            if len(enhanced_audio) > len(audio_chunk):
                enhanced_audio = enhanced_audio[:len(audio_chunk)]
            elif len(enhanced_audio) < len(audio_chunk):
                enhanced_audio = np.pad(enhanced_audio, (0, len(audio_chunk) - len(enhanced_audio)))
            
            return enhanced_audio.astype(np.int16)
            
        except Exception as e:
            print(f"ãƒã‚¤ã‚ºé™¤å»ã‚¨ãƒ©ãƒ¼: {e}")
            return audio_chunk
    
    def apply_audio_filters(self, audio_chunk):
        """éŸ³å£°ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã®é©ç”¨"""
        try:
            if len(audio_chunk) == 0:
                return audio_chunk
            
            audio_float = audio_chunk.astype(np.float32) / 32768.0
            
            # ç¾åœ¨ã®ã‚µãƒ³ãƒ—ãƒ«ãƒ¬ãƒ¼ãƒˆã«åŸºã¥ã„ã¦ãƒŠã‚¤ã‚­ã‚¹ãƒˆå‘¨æ³¢æ•°ã‚’è¨ˆç®—
            current_nyquist = self.target_fs / 2
            
            if self.highpass_freq >= current_nyquist or self.lowpass_freq >= current_nyquist:
                print(f"âš ï¸  ãƒ•ã‚£ãƒ«ã‚¿å‘¨æ³¢æ•°ãŒç¯„å›²å¤–ã§ã™ã€‚ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
                return audio_chunk
            
            # ãƒã‚¤ãƒ‘ã‚¹ãƒ•ã‚£ãƒ«ã‚¿
            try:
                high_normalized = self.highpass_freq / current_nyquist
                if 0 < high_normalized < 1:
                    sos_high = scipy.signal.butter(4, high_normalized, btype='high', output='sos')
                    filtered_audio = scipy.signal.sosfilt(sos_high, audio_float)
                else:
                    filtered_audio = audio_float
            except Exception as e:
                print(f"ãƒã‚¤ãƒ‘ã‚¹ãƒ•ã‚£ãƒ«ã‚¿ã‚¨ãƒ©ãƒ¼: {e}")
                filtered_audio = audio_float
            
            # ãƒ­ãƒ¼ãƒ‘ã‚¹ãƒ•ã‚£ãƒ«ã‚¿
            try:
                low_normalized = self.lowpass_freq / current_nyquist
                if 0 < low_normalized < 1:
                    sos_low = scipy.signal.butter(4, low_normalized, btype='low', output='sos')
                    filtered_audio = scipy.signal.sosfilt(sos_low, filtered_audio)
            except Exception as e:
                print(f"ãƒ­ãƒ¼ãƒ‘ã‚¹ãƒ•ã‚£ãƒ«ã‚¿ã‚¨ãƒ©ãƒ¼: {e}")
            
            filtered_audio = np.clip(filtered_audio, -1.0, 1.0)
            return (filtered_audio * 32767).astype(np.int16)
            
        except Exception as e:
            print(f"ã‚ªãƒ¼ãƒ‡ã‚£ã‚ªãƒ•ã‚£ãƒ«ã‚¿ã‚¨ãƒ©ãƒ¼: {e}")
            return audio_chunk
    
    def detect_voice_activity(self, audio_chunk):
        """éŸ³å£°ã‚¢ã‚¯ãƒ†ã‚£ãƒ“ãƒ†ã‚£æ¤œå‡ºï¼ˆVADï¼‰"""
        try:
            audio_float = audio_chunk.astype(np.float32) / 32768.0
            
            rms = np.sqrt(np.mean(audio_float ** 2))
            
            if len(audio_float) > 1:
                zero_crossings = np.sum(np.diff(np.sign(audio_float)) != 0) / len(audio_float)
            else:
                zero_crossings = 0
            
            try:
                if len(audio_float) >= 256:
                    f, psd = scipy.signal.welch(audio_float, fs=self.target_fs, nperseg=min(256, len(audio_float)))
                    spectral_centroid = np.sum(f * psd) / np.sum(psd) if np.sum(psd) > 0 else 0
                else:
                    spectral_centroid = 0
            except Exception as e:
                print(f"ã‚¹ãƒšã‚¯ãƒˆãƒ«é‡å¿ƒè¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}")
                spectral_centroid = 0
            
            energy_check = rms > self.vad_threshold
            spectral_check = 200 < spectral_centroid < 4000
            zcr_check = 0.01 < zero_crossings < 0.5
            
            voice_detected = energy_check and (spectral_check or zcr_check)
            
            return {
                'voice_detected': voice_detected,
                'rms': rms,
                'zero_crossings': zero_crossings,
                'spectral_centroid': spectral_centroid,
                'energy_check': energy_check,
                'spectral_check': spectral_check,
                'zcr_check': zcr_check
            }
            
        except Exception as e:
            print(f"VADæ¤œå‡ºã‚¨ãƒ©ãƒ¼: {e}")
            return {
                'voice_detected': True,
                'rms': 0,
                'zero_crossings': 0,
                'spectral_centroid': 0,
                'energy_check': False,
                'spectral_check': False,
                'zcr_check': False
            }
    
    def process_audio_chunk(self, audio_chunk):
        """éŸ³å£°ãƒãƒ£ãƒ³ã‚¯ã®å‰å‡¦ç†ã¨VAD"""
        try:
            self.total_chunks += 1
            
            if audio_chunk is None or len(audio_chunk) == 0:
                self.filtered_chunks += 1
                return None, {'voice_detected': False}
            
            # ãƒªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼ˆå¿…è¦ãªå ´åˆï¼‰
            if self.fs != self.target_fs:
                audio_chunk = self.resample_audio(audio_chunk, self.fs, self.target_fs)
            
            self.update_noise_profile(audio_chunk)
            filtered_audio = self.apply_audio_filters(audio_chunk)
            denoised_audio = self.apply_noise_reduction(filtered_audio)
            vad_result = self.detect_voice_activity(denoised_audio)
            
            if vad_result['voice_detected']:
                self.speech_chunks += 1
                return denoised_audio, vad_result
            else:
                self.filtered_chunks += 1
                return None, vad_result
                
        except Exception as e:
            print(f"éŸ³å£°ãƒãƒ£ãƒ³ã‚¯å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
            self.filtered_chunks += 1
            return None, {'voice_detected': False}
    
    def load_whisper_model(self):
        """Whisperãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿"""
        try:
            print(f"ãƒ¢ãƒ‡ãƒ«ï¼ˆ{self.model_name}ï¼‰ã‚’èª­ã¿è¾¼ã¿ä¸­...")
            print(f"ä½¿ç”¨ãƒ‡ãƒã‚¤ã‚¹: {self.device.upper()}")
            
            if self.method == "transformers":
                from transformers import WhisperProcessor, WhisperForConditionalGeneration
                
                self.processor = WhisperProcessor.from_pretrained(self.model_name)
                self.model = WhisperForConditionalGeneration.from_pretrained(self.model_name)
                
                # GPUè¨­å®š
                if self.device == "cuda":
                    self.model = self.model.cuda()
                    print(f"ğŸš€ ãƒ¢ãƒ‡ãƒ«ã‚’CUDA GPU({torch.cuda.get_device_name(0)})ã«ç§»å‹•ã—ã¾ã—ãŸ")
                elif self.device == "mps":
                    self.model = self.model.to("mps")
                    print("ğŸš€ ãƒ¢ãƒ‡ãƒ«ã‚’Apple Silicon GPUã«ç§»å‹•ã—ã¾ã—ãŸ")
                else:
                    print("ğŸ’» CPUã§ãƒ¢ãƒ‡ãƒ«ã‚’å®Ÿè¡Œã—ã¾ã™")
                
                self.model.eval()
                
                # ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–
                if self.device == "cuda":
                    torch.cuda.empty_cache()
                elif self.device == "mps":
                    torch.mps.empty_cache()
                
            else:  # OpenAI Whisper
                import whisper
                self.model = whisper.load_model(self.model_name.split('/')[-1], device=self.device)
                self.processor = None
            
            print("âœ… ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å®Œäº†!")
            return True
            
        except Exception as e:
            print(f"âŒ ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            print("ä»¥ä¸‹ã®ã‚³ãƒãƒ³ãƒ‰ã§å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ãã ã•ã„:")
            if self.method == "transformers":
                print("pip install transformers librosa torch scipy")
                if self.device == "cuda":
                    print("CUDAç‰ˆPyTorchã«ã¤ã„ã¦ã¯ä»¥ä¸‹ã‚’å‚ç…§ã—ã¦ãã ã•ã„:")
                    print("https://pytorch.org/get-started/locally/")
            else:
                print("pip install openai-whisper torch scipy")
            return False
    
    def record_audio_chunk(self):
        """æŒ‡å®šæ™‚é–“ã®éŸ³å£°ãƒãƒ£ãƒ³ã‚¯ã‚’éŒ²éŸ³ï¼ˆUbuntuå¯¾å¿œï¼‰"""
        max_retries = 3
        
        for attempt in range(max_retries):
            try:
                # Ubuntuç’°å¢ƒã§ã®éŒ²éŸ³æœ€é©åŒ–
                if self.os_name == "Linux":
                    # Linuxã§ã®éŒ²éŸ³è¨­å®šã‚’èª¿æ•´
                    recording = sd.rec(
                        int(self.chunk_duration * self.fs), 
                        samplerate=self.fs, 
                        channels=self.channels, 
                        dtype='int16', 
                        device=self.mic_device,
                        blocking=True  # Linuxã§ã®å®‰å®šæ€§å‘ä¸Š
                    )
                else:
                    recording = sd.rec(
                        int(self.chunk_duration * self.fs), 
                        samplerate=self.fs, 
                        channels=self.channels, 
                        dtype='int16', 
                        device=self.mic_device
                    )
                    sd.wait()
                
                # 2æ¬¡å…ƒé…åˆ—ã®å ´åˆã¯1æ¬¡å…ƒã«å¤‰æ›
                if recording.ndim > 1:
                    recording = recording.flatten()
                    
                return recording
                
            except Exception as e:
                print(f"éŒ²éŸ³ã‚¨ãƒ©ãƒ¼ (è©¦è¡Œ {attempt + 1}/{max_retries}): {e}")
                
                if attempt < max_retries - 1:
                    # åˆ¥ã®ã‚µãƒ³ãƒ—ãƒ«ãƒ¬ãƒ¼ãƒˆã‚’è©¦è¡Œ
                    if self.fs == 16000:
                        self.fs = 44100
                    elif self.fs == 44100:
                        self.fs = 48000
                    else:
                        self.fs = 16000
                    
                    self.nyquist_freq = self.fs / 2
                    print(f"ã‚µãƒ³ãƒ—ãƒ«ãƒ¬ãƒ¼ãƒˆã‚’ {self.fs}Hz ã«å¤‰æ›´ã—ã¦å†è©¦è¡Œ...")
                    time.sleep(1)
                else:
                    print("âŒ éŒ²éŸ³ã«å¤±æ•—ã—ã¾ã—ãŸã€‚ã‚ªãƒ¼ãƒ‡ã‚£ã‚ªãƒ‡ãƒã‚¤ã‚¹ã®è¨­å®šã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
                    if self.os_name == "Linux":
                        print("Ubuntu/Linuxã®å ´åˆã€ä»¥ä¸‹ã®ã‚³ãƒãƒ³ãƒ‰ã‚’è©¦ã—ã¦ãã ã•ã„:")
                        print("sudo apt-get install portaudio19-dev python3-pyaudio")
                        print("pulseaudio --start")
                    return None
    
    def save_audio_to_temp(self, recording):
        """éŒ²éŸ³ã—ãŸéŸ³å£°ã‚’ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜"""
        try:
            temp_file = tempfile.NamedTemporaryFile(suffix=".wav", delete=False)
            temp_filename = temp_file.name
            temp_file.close()
            
            with wave.open(temp_filename, 'wb') as wf:
                wf.setnchannels(self.channels)
                wf.setsampwidth(2)
                wf.setframerate(self.target_fs)  # Whisperç”¨ã‚µãƒ³ãƒ—ãƒ«ãƒ¬ãƒ¼ãƒˆã§ä¿å­˜
                wf.writeframes(recording.tobytes())
            
            return temp_filename
        except Exception as e:
            print(f"ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    def transcribe_audio_transformers(self, audio_file):
        """Transformersãƒ©ã‚¤ãƒ–ãƒ©ãƒªã§éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ–‡å­—èµ·ã“ã—ï¼ˆCUDA/MPSå¯¾å¿œï¼‰"""
        try:
            import librosa
            
            audio_data, _ = librosa.load(audio_file, sr=16000)
            
            if len(audio_data) < 0.5 * 16000:
                return ""
            
            inputs = self.processor(audio_data, return_tensors="pt", sampling_rate=16000)
            input_features = inputs.input_features.to(self.device)
            
            with torch.no_grad():
                forced_decoder_ids = self.processor.get_decoder_prompt_ids(language="japanese", task="transcribe")
                predicted_ids = self.model.generate(
                    input_features, 
                    forced_decoder_ids=forced_decoder_ids,
                    max_length=448,
                    num_beams=1,
                    do_sample=False
                )
            
            transcription = self.processor.batch_decode(predicted_ids, skip_special_tokens=True)[0]
            return transcription.strip()
            
        except Exception as e:
            print(f"Transformersæ–‡å­—èµ·ã“ã—ã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    def transcribe_audio_whisper(self, audio_file):
        """OpenAI Whisperã§éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ–‡å­—èµ·ã“ã—"""
        try:
            result = self.model.transcribe(audio_file, language='ja')
            return result['text'].strip()
        except Exception as e:
            print(f"Whisperæ–‡å­—èµ·ã“ã—ã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    def transcribe_audio(self, audio_file):
        """çµ±ä¸€ã•ã‚ŒãŸæ–‡å­—èµ·ã“ã—ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹"""
        if self.method == "transformers":
            return self.transcribe_audio_transformers(audio_file)
        else:
            return self.transcribe_audio_whisper(audio_file)
    
    def recording_loop(self):
        """éŒ²éŸ³ãƒ«ãƒ¼ãƒ—"""
        print("ğŸ¤ éŒ²éŸ³é–‹å§‹...")
        
        while self.is_running and not self.stop_event.is_set():
            try:
                chunk = self.record_audio_chunk()
                
                if chunk is not None:
                    processed_chunk, vad_result = self.process_audio_chunk(chunk)
                    
                    if processed_chunk is not None:
                        with self.buffer_lock:
                            self.audio_buffer.append(processed_chunk)
                            if len(self.audio_buffer) > 10:
                                self.audio_buffer.popleft()
                    else:
                        if self.total_chunks % 20 == 0:
                            speech_rate = (self.speech_chunks / self.total_chunks) * 100
                            print(f"ğŸ“Š VADçµ±è¨ˆ: éŸ³å£°ç‡ {speech_rate:.1f}% ({self.speech_chunks}/{self.total_chunks})")
                
                if not self.stop_event.is_set():
                    time.sleep(0.1)
                    
            except Exception as e:
                print(f"éŒ²éŸ³ãƒ«ãƒ¼ãƒ—ã‚¨ãƒ©ãƒ¼: {e}")
                time.sleep(1)
    
    def transcription_loop(self):
        """æ–‡å­—èµ·ã“ã—ãƒ«ãƒ¼ãƒ—"""
        print("ğŸ¤– æ–‡å­—èµ·ã“ã—é–‹å§‹...")
        
        while self.is_running and not self.stop_event.is_set():
            try:
                chunk = None
                with self.buffer_lock:
                    if self.audio_buffer:
                        chunk = self.audio_buffer.popleft()
                
                if chunk is not None:
                    start_time = time.time()
                    
                    temp_file = self.save_audio_to_temp(chunk)
                    
                    if temp_file:
                        transcription = self.transcribe_audio(temp_file)
                        
                        process_time = time.time() - start_time
                        self.processing_times.append(process_time)
                        
                        if len(self.processing_times) > 50:
                            self.processing_times.pop(0)
                        
                        if transcription and len(transcription.strip()) > 0:
                            if self.is_meaningful_transcription(transcription):
                                timestamp = datetime.now().strftime("%H:%M:%S")
                                
                                result = {
                                    'timestamp': timestamp,
                                    'text': transcription,
                                    'process_time': process_time
                                }
                                self.transcriptions.append(result)
                                
                                avg_time = sum(self.processing_times[-10:]) / min(len(self.processing_times), 10)
                                print(f"ğŸ“ [{timestamp}] ({process_time:.2f}s, å¹³å‡: {avg_time:.2f}s) {transcription}")
                            else:
                                print(f"ğŸ”‡ [{datetime.now().strftime('%H:%M:%S')}] ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°æ¸ˆã¿: \"{transcription}\"")
                        
                        try:
                            os.unlink(temp_file)
                        except:
                            pass
                        
                        # GPU ãƒ¡ãƒ¢ãƒªç®¡ç†
                        if self.device == "cuda" and len(self.transcriptions) % 10 == 0:
                            torch.cuda.empty_cache()
                        elif self.device == "mps" and len(self.transcriptions) % 10 == 0:
                            torch.mps.empty_cache()
                            
                else:
                    time.sleep(0.5)
                    
            except Exception as e:
                print(f"æ–‡å­—èµ·ã“ã—ãƒ«ãƒ¼ãƒ—ã‚¨ãƒ©ãƒ¼: {e}")
                if self.device == "cuda":
                    torch.cuda.empty_cache()
                elif self.device == "mps":
                    torch.mps.empty_cache()
                time.sleep(1)
    
    def is_meaningful_transcription(self, text):
        """æ„å‘³ã®ã‚ã‚‹æ–‡å­—èµ·ã“ã—çµæœã‹ã‚’åˆ¤å®š"""
        if not text or len(text.strip()) < 2:
            return False
        
        if len(set(text.replace(' ', ''))) < 3:
            return False
        
        noise_patterns = [
            'ã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã—ãŸ', 'ãŠç–²ã‚Œæ§˜ã§ã—ãŸ', 'ã¯ã„', 'ã‚ãƒ¼', 'ãˆãƒ¼', 'ã†ãƒ¼', 'ã‚“ãƒ¼',
            'ã€‚', 'ã€', 'ã‚', 'ãˆ', 'ã†', 'ãŠ', 'ã„'
        ]
        
        text_lower = text.lower().strip()
        if text_lower in noise_patterns or len(text_lower) <= 2:
            return False
        
        return True
    
    def start(self):
        """ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ–‡å­—èµ·ã“ã—é–‹å§‹"""
        if not self.load_whisper_model():
            return False
        
        self.is_running = True
        self.stop_event.clear()
        
        device_info = sd.query_devices(self.mic_device)
        print(f"ğŸ™ï¸  ä½¿ç”¨ãƒ‡ãƒã‚¤ã‚¹: {device_info['name']}")
        print(f"â±ï¸  ãƒãƒ£ãƒ³ã‚¯æ™‚é–“: {self.chunk_duration}ç§’")
        print(f"ğŸ”Š éŒ²éŸ³ã‚µãƒ³ãƒ—ãƒ«ãƒ¬ãƒ¼ãƒˆ: {self.fs}Hz")
        print(f"ğŸ¯ Whisperã‚µãƒ³ãƒ—ãƒ«ãƒ¬ãƒ¼ãƒˆ: {self.target_fs}Hz")
        print(f"ğŸ§  ãƒ¢ãƒ‡ãƒ«: {self.model_name}")
        print(f"ğŸ–¥ï¸  å®Ÿè¡Œç’°å¢ƒ: {self.device.upper()}")
        if self.device == "cuda":
            print(f"ğŸš€ GPU: {torch.cuda.get_device_name(0)}")
            print(f"ğŸ’¾ VRAM: {torch.cuda.get_device_properties(0).total_memory // 1024**3}GB")
        print(f"âš™ï¸  å®Ÿè£…æ–¹å¼: {self.method}")
        print(f"ğŸ¯ VADé–¾å€¤: {self.vad_threshold}")
        print(f"ğŸ”‡ ãƒã‚¤ã‚ºã‚²ãƒ¼ãƒˆ: {self.noise_gate_threshold}")
        print("\nè©±ã—å§‹ã‚ã¦ãã ã•ã„... (Ctrl+C ã§åœæ­¢)")
        print("ğŸ’¡ ç„¡éŸ³æ™‚ã‚„ãƒã‚¤ã‚ºã®ã¿ã®å ´åˆã¯æ–‡å­—èµ·ã“ã—ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™")
        print("-" * 80)
        
        self.recording_thread = threading.Thread(target=self.recording_loop)
        self.transcription_thread = threading.Thread(target=self.transcription_loop)
        
        self.recording_thread.daemon = True
        self.transcription_thread.daemon = True
        
        self.recording_thread.start()
        self.transcription_thread.start()
        
        try:
            while self.is_running:
                time.sleep(0.1)
        except KeyboardInterrupt:
            self.stop()
        
        return True
    
    def stop(self):
        """ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ–‡å­—èµ·ã“ã—åœæ­¢"""
        print(f"\nåœæ­¢å‡¦ç†ä¸­...")
        
        self.is_running = False
        self.stop_event.set()
        
        try:
            sd.stop()
        except:
            pass
        
        if self.recording_thread and self.recording_thread.is_alive():
            self.recording_thread.join(timeout=2.0)
        
        if self.transcription_thread and self.transcription_thread.is_alive():
            self.transcription_thread.join(timeout=5.0)
        
        # GPU ãƒ¡ãƒ¢ãƒªã‚¯ãƒªã‚¢
        if self.device == "cuda":
            torch.cuda.empty_cache()
        elif self.device == "mps":
            torch.mps.empty_cache()
        
        print("âœ… ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ–‡å­—èµ·ã“ã—ã‚’åœæ­¢ã—ã¾ã—ãŸã€‚")
        
        # è©³ç´°çµ±è¨ˆã‚’è¡¨ç¤º
        if self.total_chunks > 0:
            speech_rate = (self.speech_chunks / self.total_chunks) * 100
            filter_rate = (self.filtered_chunks / self.total_chunks) * 100
            
            print(f"\nğŸ“Š VADçµ±è¨ˆ:")
            print(f"  ç·ãƒãƒ£ãƒ³ã‚¯æ•°: {self.total_chunks}")
            print(f"  éŸ³å£°æ¤œå‡º: {self.speech_chunks} ({speech_rate:.1f}%)")
            print(f"  ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°: {self.filtered_chunks} ({filter_rate:.1f}%)")
        
        if self.processing_times:
            avg_time = sum(self.processing_times) / len(self.processing_times)
            min_time = min(self.processing_times)
            max_time = max(self.processing_times)
            
            print(f"\nğŸ“Š ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹çµ±è¨ˆ:")
            print(f"  å¹³å‡å‡¦ç†æ™‚é–“: {avg_time:.2f}ç§’")
            print(f"  æœ€çŸ­å‡¦ç†æ™‚é–“: {min_time:.2f}ç§’")
            print(f"  æœ€é•·å‡¦ç†æ™‚é–“: {max_time:.2f}ç§’")
            print(f"  ç·å‡¦ç†å›æ•°: {len(self.processing_times)}å›")
        
        if self.transcriptions:
            print(f"\n=== æ–‡å­—èµ·ã“ã—çµæœã‚µãƒãƒªãƒ¼ ({len(self.transcriptions)}ä»¶) ===")
            for i, result in enumerate(self.transcriptions[-10:], 1):
                print(f"{i:2d}. [{result['timestamp']}] ({result['process_time']:.2f}s) {result['text']}")
            
            if len(self.transcriptions) > 10:
                print(f"... ä»– {len(self.transcriptions) - 10} ä»¶")

def find_usb_microphone():
    """USBãƒã‚¤ã‚¯ãƒ­ãƒ•ã‚©ãƒ³ã‚’æ¤œç´¢ã—ã¦è¿”ã™ï¼ˆUbuntuå¯¾å¿œï¼‰"""
    try:
        devices = sd.query_devices()
        print("åˆ©ç”¨å¯èƒ½ãªã‚ªãƒ¼ãƒ‡ã‚£ã‚ªãƒ‡ãƒã‚¤ã‚¹:")
        for i, device in enumerate(devices):
            if device['max_input_channels'] > 0:
                print(f"  {i}: {device['name']} (å…¥åŠ›ãƒãƒ£ãƒãƒ«æ•°: {device['max_input_channels']}, "
                      f"ã‚µãƒ³ãƒ—ãƒ«ãƒ¬ãƒ¼ãƒˆ: {device['default_samplerate']}Hz)")
        
        # USBãƒã‚¤ã‚¯ã‚’è‡ªå‹•æ¤œå‡º
        usb_mics = []
        for i, device in enumerate(devices):
            if (device['max_input_channels'] > 0 and 
                ('usb' in device['name'].lower() or 'microphone' in device['name'].lower() or
                 'webcam' in device['name'].lower() or 'headset' in device['name'].lower())):
                usb_mics.append((i, device['name']))
        
        if usb_mics:
            print(f"\næ¤œå‡ºã•ã‚ŒãŸUSBãƒã‚¤ã‚¯:")
            for device_id, name in usb_mics:
                print(f"  ãƒ‡ãƒã‚¤ã‚¹ID {device_id}: {name}")
            return usb_mics[0][0]
        else:
            print("\nUSBãƒã‚¤ã‚¯ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚åˆ©ç”¨å¯èƒ½ãªå…¥åŠ›ãƒ‡ãƒã‚¤ã‚¹ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
            
            # Ubuntu/Linuxã®å ´åˆã®è¿½åŠ æƒ…å ±
            if platform.system() == "Linux":
                print("\nUbuntu/Linuxã§ã®ç¢ºèªäº‹é …:")
                print("1. ãƒã‚¤ã‚¯ãŒæ­£ã—ãæ¥ç¶šã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèª")
                print("2. ä»¥ä¸‹ã®ã‚³ãƒãƒ³ãƒ‰ã§ã‚ªãƒ¼ãƒ‡ã‚£ã‚ªã‚·ã‚¹ãƒ†ãƒ ã‚’ç¢ºèª:")
                print("   arecord -l")
                print("   pulseaudio --check")
                print("3. å¿…è¦ã«å¿œã˜ã¦ä»¥ä¸‹ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«:")
                print("   sudo apt-get install portaudio19-dev python3-pyaudio pulseaudio")
            
            return None
            
    except Exception as e:
        print(f"ã‚ªãƒ¼ãƒ‡ã‚£ã‚ªãƒ‡ãƒã‚¤ã‚¹æ¤œå‡ºã‚¨ãƒ©ãƒ¼: {e}")
        return None

transcriber_instance = None

def signal_handler(signum, frame):
    global transcriber_instance
    if transcriber_instance:
        transcriber_instance.stop()
    sys.exit(0)

def main():
    global transcriber_instance
    
    print("=== CUDAå¯¾å¿œ VAD & ãƒã‚¤ã‚ºãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚° ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ  Whisper æ–‡å­—èµ·ã“ã—ã‚·ã‚¹ãƒ†ãƒ  ===")
    print(f"å®Ÿè¡Œç’°å¢ƒ: {platform.system()} {platform.release()}")
    
    mic_device = find_usb_microphone()
    
    if mic_device is None:
        print("ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ–‡å­—èµ·ã“ã—ã‚’ä¸­æ­¢ã—ã¾ã—ãŸã€‚USBãƒã‚¤ã‚¯ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
        return
    
    # å®Ÿè£…æ–¹å¼ã‚’é¸æŠ
    print("\nå®Ÿè£…æ–¹å¼ã‚’é¸æŠã—ã¦ãã ã•ã„:")
    print("  1. OpenAI Whisper (CPU/GPU) - å®‰å®š")
    print("  2. Transformers + GPU (CUDA/MPS) - é«˜é€Ÿ [æ¨å¥¨]")
    
    while True:
        try:
            method_choice = input("é¸æŠ (1-2, ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 2): ").strip()
            if method_choice == "" or method_choice == "2":
                method = "transformers"
                break
            elif method_choice == "1":
                method = "whisper"
                break
            else:
                print("1-2ã®æ•°å­—ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
        except KeyboardInterrupt:
            print("\nçµ‚äº†ã—ã¾ã™ã€‚")
            return
    
    # ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºã‚’é¸æŠ
    print("\nWhisperãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºã‚’é¸æŠã—ã¦ãã ã•ã„:")
    if method == "transformers":
        print("  1. tiny   (æœ€é€Ÿã€ç²¾åº¦ä½)")
        print("  2. base   (ãƒãƒ©ãƒ³ã‚¹å‹) [æ¨å¥¨]")
        print("  3. small  (å°‘ã—é‡ã„ã€ç²¾åº¦è‰¯)")
        print("  4. medium (é‡ã„ã€ç²¾åº¦é«˜)")
        print("  5. large-v2 (æœ€é‡ã€æœ€é«˜ç²¾åº¦)")
        print("  6. large-v3 (æœ€æ–°ã€æœ€é«˜ç²¾åº¦) [GPUæ¨å¥¨]")
        
        model_names = {
            "1": "openai/whisper-tiny",
            "2": "openai/whisper-base",
            "3": "openai/whisper-small",
            "4": "openai/whisper-medium",
            "5": "openai/whisper-large-v2",
            "6": "openai/whisper-large-v3"
        }
    else:
        print("  1. tiny   (æœ€é€Ÿã€ç²¾åº¦ä½)")
        print("  2. base   (ãƒãƒ©ãƒ³ã‚¹å‹) [æ¨å¥¨]")
        print("  3. small  (å°‘ã—é‡ã„ã€ç²¾åº¦è‰¯)")
        print("  4. medium (é‡ã„ã€ç²¾åº¦é«˜)")
        print("  5. large  (æœ€é‡ã€æœ€é«˜ç²¾åº¦)")
        
        model_names = {
            "1": "tiny",
            "2": "base",
            "3": "small",
            "4": "medium",
            "5": "large"
        }
    
    while True:
        try:
            choice = input("é¸æŠ (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 2): ").strip()
            if choice == "" or choice == "2":
                model_name = model_names["2"]
                break
            elif choice in model_names:
                model_name = model_names[choice]
                break
            else:
                max_choice = len(model_names)
                print(f"1-{max_choice}ã®æ•°å­—ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
        except KeyboardInterrupt:
            print("\nçµ‚äº†ã—ã¾ã™ã€‚")
            return
    
    # ãƒãƒ£ãƒ³ã‚¯æ™‚é–“ã‚’é¸æŠ
    print("\néŸ³å£°ãƒãƒ£ãƒ³ã‚¯æ™‚é–“ã‚’é¸æŠã—ã¦ãã ã•ã„:")
    print("  1. 2ç§’ (é«˜åå¿œã€ä½ç²¾åº¦)")
    print("  2. 3ç§’ (ãƒãƒ©ãƒ³ã‚¹å‹) [æ¨å¥¨]")
    print("  3. 5ç§’ (ä½åå¿œã€é«˜ç²¾åº¦)")
    
    chunk_durations = {"1": 2, "2": 3, "3": 5}
    
    while True:
        try:
            chunk_choice = input("é¸æŠ (1-3, ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 2): ").strip()
            if chunk_choice == "" or chunk_choice == "2":
                chunk_duration = 3
                break
            elif chunk_choice in chunk_durations:
                chunk_duration = chunk_durations[chunk_choice]
                break
            else:
                print("1-3ã®æ•°å­—ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
        except KeyboardInterrupt:
            print("\nçµ‚äº†ã—ã¾ã™ã€‚")
            return
    
    signal.signal(signal.SIGINT, signal_handler)
    signal.signal(signal.SIGTERM, signal_handler)
    
    transcriber_instance = EnhancedRealtimeWhisperTranscriber(
        mic_device=mic_device,
        model_name=model_name,
        chunk_duration=chunk_duration,
        method=method
    )
    
    try:
        transcriber_instance.start()
    except Exception as e:
        print(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        if platform.system() == "Linux":
            print("\nUbuntuã§ã®ä¸€èˆ¬çš„ãªè§£æ±ºæ–¹æ³•:")
            print("1. sudo apt-get update && sudo apt-get install portaudio19-dev")
            print("2. pip install --upgrade sounddevice")
            print("3. pulseaudio --start")
    finally:
        if transcriber_instance:
            transcriber_instance.stop()

if __name__ == "__main__":
    main() 