import sounddevice as sd
import numpy as np
import wave
import tempfile
import os
import threading
import time
from datetime import datetime
from collections import deque
import signal
import sys
import torch
import scipy.signal

class EnhancedRealtimeWhisperTranscriber:
    def __init__(self, mic_device, model_name="openai/whisper-base", chunk_duration=3, method="transformers"):
        self.mic_device = mic_device
        self.model_name = model_name
        self.chunk_duration = chunk_duration
        self.method = method
        
        # éŸ³å£°è¨­å®š
        self.fs = 16000
        self.channels = 1
        self.nyquist_freq = self.fs / 2  # ãƒŠã‚¤ã‚­ã‚¹ãƒˆå‘¨æ³¢æ•°
        
        # GPUè¨­å®š
        self.device = self._check_device()
        
        # çŠ¶æ…‹ç®¡ç†
        self.is_running = False
        self.stop_event = threading.Event()
        
        # éŸ³å£°ãƒãƒƒãƒ•ã‚¡
        self.audio_buffer = deque()
        self.buffer_lock = threading.Lock()
        
        # ã‚¹ãƒ¬ãƒƒãƒ‰
        self.recording_thread = None
        self.transcription_thread = None
        
        # ãƒ¢ãƒ‡ãƒ«
        self.model = None
        self.processor = None
        
        # çµæœä¿å­˜
        self.transcriptions = []
        
        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹çµ±è¨ˆ
        self.processing_times = []
        
        # VADã¨ãƒã‚¤ã‚ºãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°è¨­å®šï¼ˆå®‰å…¨ãªå‘¨æ³¢æ•°ç¯„å›²ï¼‰
        self.vad_threshold = 0.02  # éŸ³å£°æ¤œå‡ºã®é–¾å€¤
        self.min_speech_duration = 0.5  # æœ€å°éŸ³å£°æ™‚é–“ï¼ˆç§’ï¼‰
        self.noise_gate_threshold = 0.01  # ãƒã‚¤ã‚ºã‚²ãƒ¼ãƒˆã®é–¾å€¤
        self.background_noise_level = 0.005  # ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ãƒã‚¤ã‚ºãƒ¬ãƒ™ãƒ«
        
        # ãƒ•ã‚£ãƒ«ã‚¿è¨­å®šï¼ˆå®‰å…¨ãªå‘¨æ³¢æ•°ç¯„å›²ï¼‰
        self.highpass_freq = max(50, 0.01 * self.nyquist_freq)  # æœ€ä½50Hzã€ã¾ãŸã¯ãƒŠã‚¤ã‚­ã‚¹ãƒˆã®1%
        self.lowpass_freq = min(7000, 0.875 * self.nyquist_freq)  # æœ€é«˜7kHzã€ã¾ãŸã¯ãƒŠã‚¤ã‚­ã‚¹ãƒˆã®87.5%
        
        # ãƒã‚¤ã‚ºãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆé©å¿œçš„ã«æ›´æ–°ï¼‰
        self.noise_profile = None
        self.noise_samples = []
        
        # çµ±è¨ˆæƒ…å ±
        self.total_chunks = 0
        self.speech_chunks = 0
        self.filtered_chunks = 0
        
        print("ğŸ¯ éŸ³å£°ã‚¢ã‚¯ãƒ†ã‚£ãƒ“ãƒ†ã‚£æ¤œå‡ºï¼ˆVADï¼‰æ©Ÿèƒ½ã‚’æœ‰åŠ¹ã«ã—ã¾ã—ãŸ")
        print("ğŸ”‡ ãƒã‚¤ã‚ºãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°æ©Ÿèƒ½ã‚’æœ‰åŠ¹ã«ã—ã¾ã—ãŸ")
        print(f"ğŸ”§ ãƒ•ã‚£ãƒ«ã‚¿å‘¨æ³¢æ•°ç¯„å›²: {self.highpass_freq:.0f}Hz - {self.lowpass_freq:.0f}Hz")
    
    def _check_device(self):
        """GPUåˆ©ç”¨å¯èƒ½æ€§ã‚’ãƒã‚§ãƒƒã‚¯"""
        if torch.backends.mps.is_available():
            print("âœ… Apple Silicon GPU (MPS) ãŒåˆ©ç”¨å¯èƒ½ã§ã™")
            return "mps"
        elif torch.cuda.is_available():
            print("âœ… NVIDIA GPU (CUDA) ãŒåˆ©ç”¨å¯èƒ½ã§ã™")
            return "cuda"
        else:
            print("âš ï¸  GPU ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚CPUã‚’ä½¿ç”¨ã—ã¾ã™")
            return "cpu"
    
    def update_noise_profile(self, audio_chunk):
        """ãƒã‚¤ã‚ºãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ã‚’é©å¿œçš„ã«æ›´æ–°"""
        try:
            # éŸ³å£°ãŒãªã„å ´åˆã®ã‚µãƒ³ãƒ—ãƒ«ã‚’ãƒã‚¤ã‚ºã¨ã—ã¦å­¦ç¿’
            rms = np.sqrt(np.mean(audio_chunk.astype(np.float32) ** 2))
            
            if rms < self.noise_gate_threshold:
                self.noise_samples.append(audio_chunk)
                
                # æœ€æ–°50ã‚µãƒ³ãƒ—ãƒ«ã®ã¿ä¿æŒ
                if len(self.noise_samples) > 50:
                    self.noise_samples.pop(0)
                
                # ãƒã‚¤ã‚ºãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ›´æ–°
                if len(self.noise_samples) >= 5:
                    combined_noise = np.concatenate(self.noise_samples)
                    self.noise_profile = np.mean(combined_noise)
                    self.background_noise_level = np.std(combined_noise) * 2
        except Exception as e:
            print(f"ãƒã‚¤ã‚ºãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«æ›´æ–°ã‚¨ãƒ©ãƒ¼: {e}")
    
    def apply_noise_reduction(self, audio_chunk):
        """ã‚¹ãƒšã‚¯ãƒˆãƒ«æ¸›ç®—ã«ã‚ˆã‚‹ãƒã‚¤ã‚ºé™¤å»ï¼ˆã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°å¼·åŒ–ï¼‰"""
        try:
            # å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã®æ¤œè¨¼
            if len(audio_chunk) < 256:
                return audio_chunk
            
            # STFTã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å®‰å…¨ã«è¨­å®š
            nperseg = min(256, len(audio_chunk) // 4)
            if nperseg < 4:
                return audio_chunk
            
            # çŸ­æ™‚é–“ãƒ•ãƒ¼ãƒªã‚¨å¤‰æ›
            f, t, stft = scipy.signal.stft(audio_chunk, fs=self.fs, nperseg=nperseg)
            
            # ã‚¹ãƒšã‚¯ãƒˆãƒ©ãƒ ã®å¤§ãã•ã¨ä½ç›¸
            magnitude = np.abs(stft)
            phase = np.angle(stft)
            
            # ãƒã‚¤ã‚ºãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã™ã‚‹å ´åˆã€ã‚¹ãƒšã‚¯ãƒˆãƒ«æ¸›ç®—ã‚’é©ç”¨
            if self.noise_profile is not None:
                # ãƒã‚¤ã‚ºãƒ¬ãƒ™ãƒ«ã®æ¨å®š
                noise_power = self.background_noise_level ** 2
                signal_power = magnitude ** 2
                
                # ã‚¦ã‚£ãƒ¼ãƒŠãƒ¼ãƒ•ã‚£ãƒ«ã‚¿ã®é©ç”¨ï¼ˆå®‰å…¨ãªç¯„å›²ã§ï¼‰
                alpha = 1.5  # æ¸›ç®—ä¿‚æ•°ã‚’æ§ãˆã‚ã«
                enhanced_magnitude = magnitude * np.maximum(
                    0.1, 1 - alpha * noise_power / (signal_power + 1e-10)
                )
            else:
                enhanced_magnitude = magnitude
            
            # é€†çŸ­æ™‚é–“ãƒ•ãƒ¼ãƒªã‚¨å¤‰æ›
            enhanced_stft = enhanced_magnitude * np.exp(1j * phase)
            _, enhanced_audio = scipy.signal.istft(enhanced_stft, fs=self.fs, nperseg=nperseg)
            
            # å…ƒã®é•·ã•ã«åˆã‚ã›ã¦ãƒˆãƒªãƒŸãƒ³ã‚°
            if len(enhanced_audio) > len(audio_chunk):
                enhanced_audio = enhanced_audio[:len(audio_chunk)]
            elif len(enhanced_audio) < len(audio_chunk):
                # ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°
                enhanced_audio = np.pad(enhanced_audio, (0, len(audio_chunk) - len(enhanced_audio)))
            
            return enhanced_audio.astype(np.int16)
            
        except Exception as e:
            print(f"ãƒã‚¤ã‚ºé™¤å»ã‚¨ãƒ©ãƒ¼: {e}")
            return audio_chunk
    
    def apply_audio_filters(self, audio_chunk):
        """éŸ³å£°ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã®é©ç”¨ï¼ˆã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°å¼·åŒ–ï¼‰"""
        try:
            # å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã®æ¤œè¨¼
            if len(audio_chunk) == 0:
                return audio_chunk
            
            # æ­£è¦åŒ–
            audio_float = audio_chunk.astype(np.float32) / 32768.0
            
            # ãƒ•ã‚£ãƒ«ã‚¿å‘¨æ³¢æ•°ã®å¦¥å½“æ€§ã‚’ãƒã‚§ãƒƒã‚¯
            if self.highpass_freq >= self.nyquist_freq or self.lowpass_freq >= self.nyquist_freq:
                print(f"âš ï¸  ãƒ•ã‚£ãƒ«ã‚¿å‘¨æ³¢æ•°ãŒç¯„å›²å¤–ã§ã™ã€‚ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
                return audio_chunk
            
            # ãƒã‚¤ãƒ‘ã‚¹ãƒ•ã‚£ãƒ«ã‚¿ï¼ˆä½å‘¨æ³¢ãƒã‚¤ã‚ºé™¤å»ï¼‰
            try:
                # æ­£è¦åŒ–ã•ã‚ŒãŸå‘¨æ³¢æ•°ã‚’ä½¿ç”¨
                high_normalized = self.highpass_freq / self.nyquist_freq
                if 0 < high_normalized < 1:
                    sos_high = scipy.signal.butter(4, high_normalized, btype='high', output='sos')
                    filtered_audio = scipy.signal.sosfilt(sos_high, audio_float)
                else:
                    filtered_audio = audio_float
            except Exception as e:
                print(f"ãƒã‚¤ãƒ‘ã‚¹ãƒ•ã‚£ãƒ«ã‚¿ã‚¨ãƒ©ãƒ¼: {e}")
                filtered_audio = audio_float
            
            # ãƒ­ãƒ¼ãƒ‘ã‚¹ãƒ•ã‚£ãƒ«ã‚¿ï¼ˆé«˜å‘¨æ³¢ãƒã‚¤ã‚ºé™¤å»ï¼‰
            try:
                # æ­£è¦åŒ–ã•ã‚ŒãŸå‘¨æ³¢æ•°ã‚’ä½¿ç”¨
                low_normalized = self.lowpass_freq / self.nyquist_freq
                if 0 < low_normalized < 1:
                    sos_low = scipy.signal.butter(4, low_normalized, btype='low', output='sos')
                    filtered_audio = scipy.signal.sosfilt(sos_low, filtered_audio)
            except Exception as e:
                print(f"ãƒ­ãƒ¼ãƒ‘ã‚¹ãƒ•ã‚£ãƒ«ã‚¿ã‚¨ãƒ©ãƒ¼: {e}")
            
            # æ­£è¦åŒ–ã—ã¦å…ƒã®ã‚¹ã‚±ãƒ¼ãƒ«ã«æˆ»ã™
            filtered_audio = np.clip(filtered_audio, -1.0, 1.0)
            return (filtered_audio * 32767).astype(np.int16)
            
        except Exception as e:
            print(f"ã‚ªãƒ¼ãƒ‡ã‚£ã‚ªãƒ•ã‚£ãƒ«ã‚¿ã‚¨ãƒ©ãƒ¼: {e}")
            return audio_chunk
    
    def detect_voice_activity(self, audio_chunk):
        """éŸ³å£°ã‚¢ã‚¯ãƒ†ã‚£ãƒ“ãƒ†ã‚£æ¤œå‡ºï¼ˆVADï¼‰"""
        try:
            # éŸ³å£°ã‚’æ­£è¦åŒ–
            audio_float = audio_chunk.astype(np.float32) / 32768.0
            
            # 1. RMSï¼ˆRoot Mean Squareï¼‰ã‚¨ãƒãƒ«ã‚®ãƒ¼è¨ˆç®—
            rms = np.sqrt(np.mean(audio_float ** 2))
            
            # 2. ã‚¼ãƒ­ã‚¯ãƒ­ãƒƒã‚·ãƒ³ã‚°ç‡è¨ˆç®—
            if len(audio_float) > 1:
                zero_crossings = np.sum(np.diff(np.sign(audio_float)) != 0) / len(audio_float)
            else:
                zero_crossings = 0
            
            # 3. ã‚¹ãƒšã‚¯ãƒˆãƒ«é‡å¿ƒè¨ˆç®—
            try:
                if len(audio_float) >= 256:
                    f, psd = scipy.signal.welch(audio_float, fs=self.fs, nperseg=min(256, len(audio_float)))
                    spectral_centroid = np.sum(f * psd) / np.sum(psd) if np.sum(psd) > 0 else 0
                else:
                    spectral_centroid = 0
            except Exception as e:
                print(f"ã‚¹ãƒšã‚¯ãƒˆãƒ«é‡å¿ƒè¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}")
                spectral_centroid = 0
            
            # 4. ç·åˆçš„ãªéŸ³å£°åˆ¤å®š
            energy_check = rms > self.vad_threshold
            spectral_check = 200 < spectral_centroid < 4000  # äººé–“ã®éŸ³å£°å‘¨æ³¢æ•°ç¯„å›²
            zcr_check = 0.01 < zero_crossings < 0.5  # é©åˆ‡ãªã‚¼ãƒ­ã‚¯ãƒ­ãƒƒã‚·ãƒ³ã‚°ç‡
            
            # ãƒ‡ãƒãƒƒã‚°æƒ…å ±
            voice_detected = energy_check and (spectral_check or zcr_check)
            
            return {
                'voice_detected': voice_detected,
                'rms': rms,
                'zero_crossings': zero_crossings,
                'spectral_centroid': spectral_centroid,
                'energy_check': energy_check,
                'spectral_check': spectral_check,
                'zcr_check': zcr_check
            }
            
        except Exception as e:
            print(f"VADæ¤œå‡ºã‚¨ãƒ©ãƒ¼: {e}")
            # ã‚¨ãƒ©ãƒ¼æ™‚ã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§éŸ³å£°ã¨ã—ã¦æ‰±ã†
            return {
                'voice_detected': True,
                'rms': 0,
                'zero_crossings': 0,
                'spectral_centroid': 0,
                'energy_check': False,
                'spectral_check': False,
                'zcr_check': False
            }
    
    def process_audio_chunk(self, audio_chunk):
        """éŸ³å£°ãƒãƒ£ãƒ³ã‚¯ã®å‰å‡¦ç†ã¨VAD"""
        try:
            self.total_chunks += 1
            
            # å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã®æ¤œè¨¼
            if audio_chunk is None or len(audio_chunk) == 0:
                self.filtered_chunks += 1
                return None, {'voice_detected': False}
            
            # ãƒã‚¤ã‚ºãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ã®æ›´æ–°
            self.update_noise_profile(audio_chunk)
            
            # ã‚ªãƒ¼ãƒ‡ã‚£ã‚ªãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°é©ç”¨
            filtered_audio = self.apply_audio_filters(audio_chunk)
            
            # ãƒã‚¤ã‚ºé™¤å»é©ç”¨
            denoised_audio = self.apply_noise_reduction(filtered_audio)
            
            # VADå®Ÿè¡Œ
            vad_result = self.detect_voice_activity(denoised_audio)
            
            if vad_result['voice_detected']:
                self.speech_chunks += 1
                return denoised_audio, vad_result
            else:
                self.filtered_chunks += 1
                return None, vad_result
                
        except Exception as e:
            print(f"éŸ³å£°ãƒãƒ£ãƒ³ã‚¯å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
            self.filtered_chunks += 1
            return None, {'voice_detected': False}
    
    def load_whisper_model(self):
        """Whisperãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿"""
        try:
            print(f"ãƒ¢ãƒ‡ãƒ«ï¼ˆ{self.model_name}ï¼‰ã‚’èª­ã¿è¾¼ã¿ä¸­...")
            print(f"ä½¿ç”¨ãƒ‡ãƒã‚¤ã‚¹: {self.device.upper()}")
            
            if self.method == "transformers":
                from transformers import WhisperProcessor, WhisperForConditionalGeneration
                
                self.processor = WhisperProcessor.from_pretrained(self.model_name)
                self.model = WhisperForConditionalGeneration.from_pretrained(self.model_name)
                self.model = self.model.to(self.device)
                self.model.eval()
                
                if self.device == "mps":
                    torch.mps.empty_cache()
                
            else:  # OpenAI Whisper
                import whisper
                self.model = whisper.load_model(self.model_name.split('/')[-1], device=self.device)
                self.processor = None
            
            print("âœ… ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å®Œäº†!")
            return True
            
        except Exception as e:
            print(f"âŒ ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            print("ä»¥ä¸‹ã®ã‚³ãƒãƒ³ãƒ‰ã§å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ãã ã•ã„:")
            if self.method == "transformers":
                print("pip install transformers librosa torch scipy")
            else:
                print("pip install openai-whisper torch scipy")
            return False
    
    def record_audio_chunk(self):
        """æŒ‡å®šæ™‚é–“ã®éŸ³å£°ãƒãƒ£ãƒ³ã‚¯ã‚’éŒ²éŸ³"""
        try:
            recording = sd.rec(int(self.chunk_duration * self.fs), 
                             samplerate=self.fs, 
                             channels=self.channels, 
                             dtype='int16', 
                             device=self.mic_device)
            sd.wait()
            
            # 2æ¬¡å…ƒé…åˆ—ã®å ´åˆã¯1æ¬¡å…ƒã«å¤‰æ›
            if recording.ndim > 1:
                recording = recording.flatten()
                
            return recording
        except Exception as e:
            print(f"éŒ²éŸ³ã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    def save_audio_to_temp(self, recording):
        """éŒ²éŸ³ã—ãŸéŸ³å£°ã‚’ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜"""
        try:
            temp_file = tempfile.NamedTemporaryFile(suffix=".wav", delete=False)
            temp_filename = temp_file.name
            temp_file.close()
            
            with wave.open(temp_filename, 'wb') as wf:
                wf.setnchannels(self.channels)
                wf.setsampwidth(2)
                wf.setframerate(self.fs)
                wf.writeframes(recording.tobytes())
            
            return temp_filename
        except Exception as e:
            print(f"ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    def transcribe_audio_transformers(self, audio_file):
        """Transformersãƒ©ã‚¤ãƒ–ãƒ©ãƒªã§éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ–‡å­—èµ·ã“ã—ï¼ˆMPSå¯¾å¿œï¼‰"""
        try:
            import librosa
            
            audio_data, _ = librosa.load(audio_file, sr=16000)
            
            if len(audio_data) < 0.5 * 16000:
                return ""
            
            inputs = self.processor(audio_data, return_tensors="pt", sampling_rate=16000)
            input_features = inputs.input_features.to(self.device)
            
            with torch.no_grad():
                forced_decoder_ids = self.processor.get_decoder_prompt_ids(language="japanese", task="transcribe")
                predicted_ids = self.model.generate(
                    input_features, 
                    forced_decoder_ids=forced_decoder_ids,
                    max_length=448,
                    num_beams=1,
                    do_sample=False
                )
            
            transcription = self.processor.batch_decode(predicted_ids, skip_special_tokens=True)[0]
            return transcription.strip()
            
        except Exception as e:
            print(f"Transformersæ–‡å­—èµ·ã“ã—ã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    def transcribe_audio_whisper(self, audio_file):
        """OpenAI Whisperã§éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ–‡å­—èµ·ã“ã—"""
        try:
            result = self.model.transcribe(audio_file, language='ja')
            return result['text'].strip()
        except Exception as e:
            print(f"Whisperæ–‡å­—èµ·ã“ã—ã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    def transcribe_audio(self, audio_file):
        """çµ±ä¸€ã•ã‚ŒãŸæ–‡å­—èµ·ã“ã—ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹"""
        if self.method == "transformers":
            return self.transcribe_audio_transformers(audio_file)
        else:
            return self.transcribe_audio_whisper(audio_file)
    
    def recording_loop(self):
        """éŒ²éŸ³ãƒ«ãƒ¼ãƒ—"""
        print("ğŸ¤ éŒ²éŸ³é–‹å§‹...")
        
        while self.is_running and not self.stop_event.is_set():
            try:
                chunk = self.record_audio_chunk()
                
                if chunk is not None:
                    # éŸ³å£°å‰å‡¦ç†ã¨VAD
                    processed_chunk, vad_result = self.process_audio_chunk(chunk)
                    
                    if processed_chunk is not None:
                        # éŸ³å£°ãŒæ¤œå‡ºã•ã‚ŒãŸå ´åˆã®ã¿ãƒãƒƒãƒ•ã‚¡ã«è¿½åŠ 
                        with self.buffer_lock:
                            self.audio_buffer.append(processed_chunk)
                            if len(self.audio_buffer) > 10:
                                self.audio_buffer.popleft()
                    else:
                        # éŸ³å£°ãŒæ¤œå‡ºã•ã‚Œãªã‹ã£ãŸå ´åˆã®è©³ç´°ãƒ­ã‚°ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
                        if self.total_chunks % 20 == 0:  # 20ãƒãƒ£ãƒ³ã‚¯ã”ã¨ã«çµ±è¨ˆè¡¨ç¤º
                            speech_rate = (self.speech_chunks / self.total_chunks) * 100
                            print(f"ğŸ“Š VADçµ±è¨ˆ: éŸ³å£°ç‡ {speech_rate:.1f}% ({self.speech_chunks}/{self.total_chunks})")
                
                if not self.stop_event.is_set():
                    time.sleep(0.1)
                    
            except Exception as e:
                print(f"éŒ²éŸ³ãƒ«ãƒ¼ãƒ—ã‚¨ãƒ©ãƒ¼: {e}")
                time.sleep(1)  # ã‚¨ãƒ©ãƒ¼æ™‚ã¯å°‘ã—å¾…æ©Ÿ
    
    def transcription_loop(self):
        """æ–‡å­—èµ·ã“ã—ãƒ«ãƒ¼ãƒ—"""
        print("ğŸ¤– æ–‡å­—èµ·ã“ã—é–‹å§‹...")
        
        while self.is_running and not self.stop_event.is_set():
            try:
                chunk = None
                with self.buffer_lock:
                    if self.audio_buffer:
                        chunk = self.audio_buffer.popleft()
                
                if chunk is not None:
                    start_time = time.time()
                    
                    temp_file = self.save_audio_to_temp(chunk)
                    
                    if temp_file:
                        transcription = self.transcribe_audio(temp_file)
                        
                        process_time = time.time() - start_time
                        self.processing_times.append(process_time)
                        
                        if len(self.processing_times) > 50:
                            self.processing_times.pop(0)
                        
                        # æ–‡å­—èµ·ã“ã—çµæœã®ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
                        if transcription and len(transcription.strip()) > 0:
                            # ç„¡æ„å‘³ãªçµæœã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
                            if self.is_meaningful_transcription(transcription):
                                timestamp = datetime.now().strftime("%H:%M:%S")
                                
                                result = {
                                    'timestamp': timestamp,
                                    'text': transcription,
                                    'process_time': process_time
                                }
                                self.transcriptions.append(result)
                                
                                avg_time = sum(self.processing_times[-10:]) / min(len(self.processing_times), 10)
                                print(f"ğŸ“ [{timestamp}] ({process_time:.2f}s, å¹³å‡: {avg_time:.2f}s) {transcription}")
                            else:
                                print(f"ğŸ”‡ [{datetime.now().strftime('%H:%M:%S')}] ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°æ¸ˆã¿: \"{transcription}\"")
                        
                        try:
                            os.unlink(temp_file)
                        except:
                            pass
                        
                        if self.device == "mps" and len(self.transcriptions) % 10 == 0:
                            torch.mps.empty_cache()
                            
                else:
                    time.sleep(0.5)
                    
            except Exception as e:
                print(f"æ–‡å­—èµ·ã“ã—ãƒ«ãƒ¼ãƒ—ã‚¨ãƒ©ãƒ¼: {e}")
                if self.device == "mps":
                    torch.mps.empty_cache()
                time.sleep(1)  # ã‚¨ãƒ©ãƒ¼æ™‚ã¯å°‘ã—å¾…æ©Ÿ
    
    def is_meaningful_transcription(self, text):
        """æ„å‘³ã®ã‚ã‚‹æ–‡å­—èµ·ã“ã—çµæœã‹ã‚’åˆ¤å®š"""
        if not text or len(text.strip()) < 2:
            return False
        
        # ç¹°ã‚Šè¿”ã—ãƒ‘ã‚¿ãƒ¼ãƒ³ã®ãƒã‚§ãƒƒã‚¯
        if len(set(text.replace(' ', ''))) < 3:  # æ–‡å­—ã®ç¨®é¡ãŒå°‘ãªã™ãã‚‹
            return False
        
        # ä¸€èˆ¬çš„ãªãƒã‚¤ã‚ºæ–‡å­—åˆ—ã®ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        noise_patterns = [
            'ã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã—ãŸ',  # ã‚ˆãã‚ã‚‹èª¤èªè­˜
            'ãŠç–²ã‚Œæ§˜ã§ã—ãŸ',
            'ã¯ã„',
            'ã‚ãƒ¼',
            'ãˆãƒ¼',
            'ã†ãƒ¼',
            'ã‚“ãƒ¼',
            'ã€‚',
            'ã€',
            'ã‚',
            'ãˆ',
            'ã†',
            'ãŠ'
        ]
        
        text_lower = text.lower().strip()
        if text_lower in noise_patterns or len(text_lower) <= 2:
            return False
        
        return True
    
    def start(self):
        """ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ–‡å­—èµ·ã“ã—é–‹å§‹"""
        if not self.load_whisper_model():
            return False
        
        self.is_running = True
        self.stop_event.clear()
        
        print(f"ğŸ™ï¸  ä½¿ç”¨ãƒ‡ãƒã‚¤ã‚¹: {sd.query_devices(self.mic_device)['name']}")
        print(f"â±ï¸  ãƒãƒ£ãƒ³ã‚¯æ™‚é–“: {self.chunk_duration}ç§’")
        print(f"ğŸ”Š ã‚µãƒ³ãƒ—ãƒ«ãƒ¬ãƒ¼ãƒˆ: {self.fs}Hz")
        print(f"ğŸ§  ãƒ¢ãƒ‡ãƒ«: {self.model_name}")
        print(f"ğŸ–¥ï¸  å®Ÿè¡Œç’°å¢ƒ: {self.device.upper()}")
        print(f"âš™ï¸  å®Ÿè£…æ–¹å¼: {self.method}")
        print(f"ğŸ¯ VADé–¾å€¤: {self.vad_threshold}")
        print(f"ğŸ”‡ ãƒã‚¤ã‚ºã‚²ãƒ¼ãƒˆ: {self.noise_gate_threshold}")
        print("\nè©±ã—å§‹ã‚ã¦ãã ã•ã„... (Ctrl+C ã§åœæ­¢)")
        print("ğŸ’¡ ç„¡éŸ³æ™‚ã‚„ãƒã‚¤ã‚ºã®ã¿ã®å ´åˆã¯æ–‡å­—èµ·ã“ã—ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™")
        print("-" * 70)
        
        self.recording_thread = threading.Thread(target=self.recording_loop)
        self.transcription_thread = threading.Thread(target=self.transcription_loop)
        
        self.recording_thread.daemon = True
        self.transcription_thread.daemon = True
        
        self.recording_thread.start()
        self.transcription_thread.start()
        
        try:
            while self.is_running:
                time.sleep(0.1)
        except KeyboardInterrupt:
            self.stop()
        
        return True
    
    def stop(self):
        """ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ–‡å­—èµ·ã“ã—åœæ­¢"""
        print(f"\nåœæ­¢å‡¦ç†ä¸­...")
        
        self.is_running = False
        self.stop_event.set()
        
        try:
            sd.stop()
        except:
            pass
        
        if self.recording_thread and self.recording_thread.is_alive():
            self.recording_thread.join(timeout=2.0)
        
        if self.transcription_thread and self.transcription_thread.is_alive():
            self.transcription_thread.join(timeout=5.0)
        
        if self.device == "mps":
            torch.mps.empty_cache()
        
        print("âœ… ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ–‡å­—èµ·ã“ã—ã‚’åœæ­¢ã—ã¾ã—ãŸã€‚")
        
        # è©³ç´°çµ±è¨ˆã‚’è¡¨ç¤º
        if self.total_chunks > 0:
            speech_rate = (self.speech_chunks / self.total_chunks) * 100
            filter_rate = (self.filtered_chunks / self.total_chunks) * 100
            
            print(f"\nğŸ“Š VADçµ±è¨ˆ:")
            print(f"  ç·ãƒãƒ£ãƒ³ã‚¯æ•°: {self.total_chunks}")
            print(f"  éŸ³å£°æ¤œå‡º: {self.speech_chunks} ({speech_rate:.1f}%)")
            print(f"  ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°: {self.filtered_chunks} ({filter_rate:.1f}%)")
        
        if self.processing_times:
            avg_time = sum(self.processing_times) / len(self.processing_times)
            min_time = min(self.processing_times)
            max_time = max(self.processing_times)
            
            print(f"\nğŸ“Š ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹çµ±è¨ˆ:")
            print(f"  å¹³å‡å‡¦ç†æ™‚é–“: {avg_time:.2f}ç§’")
            print(f"  æœ€çŸ­å‡¦ç†æ™‚é–“: {min_time:.2f}ç§’")
            print(f"  æœ€é•·å‡¦ç†æ™‚é–“: {max_time:.2f}ç§’")
            print(f"  ç·å‡¦ç†å›æ•°: {len(self.processing_times)}å›")
        
        if self.transcriptions:
            print(f"\n=== æ–‡å­—èµ·ã“ã—çµæœã‚µãƒãƒªãƒ¼ ({len(self.transcriptions)}ä»¶) ===")
            for i, result in enumerate(self.transcriptions[-10:], 1):
                print(f"{i:2d}. [{result['timestamp']}] ({result['process_time']:.2f}s) {result['text']}")
            
            if len(self.transcriptions) > 10:
                print(f"... ä»– {len(self.transcriptions) - 10} ä»¶")

def find_usb_microphone():
    """USBãƒã‚¤ã‚¯ãƒ­ãƒ•ã‚©ãƒ³ã‚’æ¤œç´¢ã—ã¦è¿”ã™"""
    devices = sd.query_devices()
    print("åˆ©ç”¨å¯èƒ½ãªã‚ªãƒ¼ãƒ‡ã‚£ã‚ªãƒ‡ãƒã‚¤ã‚¹:")
    for i, device in enumerate(devices):
        if device['max_input_channels'] > 0:
            print(f"  {i}: {device['name']} (å…¥åŠ›ãƒãƒ£ãƒãƒ«æ•°: {device['max_input_channels']})")
    
    usb_mics = []
    for i, device in enumerate(devices):
        if (device['max_input_channels'] > 0 and 
            ('usb' in device['name'].lower() or 'microphone' in device['name'].lower())):
            usb_mics.append((i, device['name']))
    
    if usb_mics:
        print(f"\næ¤œå‡ºã•ã‚ŒãŸUSBãƒã‚¤ã‚¯:")
        for device_id, name in usb_mics:
            print(f"  ãƒ‡ãƒã‚¤ã‚¹ID {device_id}: {name}")
        return usb_mics[0][0]
    else:
        print("\nUSBãƒã‚¤ã‚¯ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚åˆ©ç”¨å¯èƒ½ãªå…¥åŠ›ãƒ‡ãƒã‚¤ã‚¹ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
        return None

transcriber_instance = None

def signal_handler(signum, frame):
    global transcriber_instance
    if transcriber_instance:
        transcriber_instance.stop()
    sys.exit(0)

def main():
    global transcriber_instance
    
    print("=== VAD & ãƒã‚¤ã‚ºãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å¯¾å¿œ ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ  Whisper æ–‡å­—èµ·ã“ã—ã‚·ã‚¹ãƒ†ãƒ  (ä¿®æ­£ç‰ˆ) ===")
    
    mic_device = find_usb_microphone()
    
    if mic_device is None:
        print("ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ–‡å­—èµ·ã“ã—ã‚’ä¸­æ­¢ã—ã¾ã—ãŸã€‚USBãƒã‚¤ã‚¯ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
        return
    
    # å®Ÿè£…æ–¹å¼ã‚’é¸æŠ
    print("\nå®Ÿè£…æ–¹å¼ã‚’é¸æŠã—ã¦ãã ã•ã„:")
    print("  1. OpenAI Whisper (CPU/MPS) - å®‰å®š")
    print("  2. Transformers + MPS (GPU) - é«˜é€Ÿ [æ¨å¥¨]")
    
    while True:
        try:
            method_choice = input("é¸æŠ (1-2, ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 2): ").strip()
            if method_choice == "" or method_choice == "2":
                method = "transformers"
                break
            elif method_choice == "1":
                method = "whisper"
                break
            else:
                print("1-2ã®æ•°å­—ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
        except KeyboardInterrupt:
            print("\nçµ‚äº†ã—ã¾ã™ã€‚")
            return
    
    # ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºã‚’é¸æŠ
    print("\nWhisperãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºã‚’é¸æŠã—ã¦ãã ã•ã„:")
    if method == "transformers":
        print("  1. tiny   (æœ€é€Ÿã€ç²¾åº¦ä½)")
        print("  2. base   (ãƒãƒ©ãƒ³ã‚¹å‹) [æ¨å¥¨]")
        print("  3. small  (å°‘ã—é‡ã„ã€ç²¾åº¦è‰¯)")
        print("  4. medium (é‡ã„ã€ç²¾åº¦é«˜)")
        print("  5. large-v2 (æœ€é‡ã€æœ€é«˜ç²¾åº¦)")
        print("  6. large-v3 (æœ€æ–°ã€æœ€é«˜ç²¾åº¦) [MPSæ¨å¥¨]")
        
        model_names = {
            "1": "openai/whisper-tiny",
            "2": "openai/whisper-base",
            "3": "openai/whisper-small",
            "4": "openai/whisper-medium",
            "5": "openai/whisper-large-v2",
            "6": "openai/whisper-large-v3"
        }
    else:
        print("  1. tiny   (æœ€é€Ÿã€ç²¾åº¦ä½)")
        print("  2. base   (ãƒãƒ©ãƒ³ã‚¹å‹) [æ¨å¥¨]")
        print("  3. small  (å°‘ã—é‡ã„ã€ç²¾åº¦è‰¯)")
        print("  4. medium (é‡ã„ã€ç²¾åº¦é«˜)")
        print("  5. large  (æœ€é‡ã€æœ€é«˜ç²¾åº¦)")
        
        model_names = {
            "1": "tiny",
            "2": "base",
            "3": "small",
            "4": "medium",
            "5": "large"
        }
    
    while True:
        try:
            choice = input("é¸æŠ (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 2): ").strip()
            if choice == "" or choice == "2":
                model_name = model_names["2"]
                break
            elif choice in model_names:
                model_name = model_names[choice]
                break
            else:
                max_choice = len(model_names)
                print(f"1-{max_choice}ã®æ•°å­—ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
        except KeyboardInterrupt:
            print("\nçµ‚äº†ã—ã¾ã™ã€‚")
            return
    
    # ãƒãƒ£ãƒ³ã‚¯æ™‚é–“ã‚’é¸æŠ
    print("\néŸ³å£°ãƒãƒ£ãƒ³ã‚¯æ™‚é–“ã‚’é¸æŠã—ã¦ãã ã•ã„:")
    print("  1. 2ç§’ (é«˜åå¿œã€ä½ç²¾åº¦)")
    print("  2. 3ç§’ (ãƒãƒ©ãƒ³ã‚¹å‹) [æ¨å¥¨]")
    print("  3. 5ç§’ (ä½åå¿œã€é«˜ç²¾åº¦)")
    
    chunk_durations = {"1": 2, "2": 3, "3": 5}
    
    while True:
        try:
            chunk_choice = input("é¸æŠ (1-3, ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 2): ").strip()
            if chunk_choice == "" or chunk_choice == "2":
                chunk_duration = 3
                break
            elif chunk_choice in chunk_durations:
                chunk_duration = chunk_durations[chunk_choice]
                break
            else:
                print("1-3ã®æ•°å­—ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
        except KeyboardInterrupt:
            print("\nçµ‚äº†ã—ã¾ã™ã€‚")
            return
    
    signal.signal(signal.SIGINT, signal_handler)
    signal.signal(signal.SIGTERM, signal_handler)
    
    transcriber_instance = EnhancedRealtimeWhisperTranscriber(
        mic_device=mic_device,
        model_name=model_name,
        chunk_duration=chunk_duration,
        method=method
    )
    
    try:
        transcriber_instance.start()
    except Exception as e:
        print(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
    finally:
        if transcriber_instance:
            transcriber_instance.stop()

if __name__ == "__main__":
    main() 